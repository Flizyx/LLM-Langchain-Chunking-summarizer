{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7qTk2FtrwBtuvHgZ4jrSx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Flizyx/LLM-Langchain-Chunking-summarizer/blob/main/AI_Engineer_Test_Generative_Transcript_Transformation_(chunking_openai).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Install Dependencies ---\n",
        "\n",
        "#!pip install -q langchain transformers pypdf PyPDF2 gradio langchain-community\n",
        "!pip install -q langchain openai tiktoken pypdf PyPDF2 gradio langchain-community"
      ],
      "metadata": {
        "id": "PfeAyfdO_VG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR_API_KEY\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\""
      ],
      "metadata": {
        "id": "Y7n7AzJbA7RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hello"
      ],
      "metadata": {
        "id": "UTRduUSxb6dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "\n",
        "#######################################\n",
        "# 2) Initialize ChatOpenAI\n",
        "#######################################\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=3000  # Increase for longer outputs\n",
        ")\n",
        "\n",
        "#######################################\n",
        "# 3) Prompts\n",
        "#######################################\n",
        "SUMMARY_PROMPT = \"\"\"\n",
        "You are a world-class summarizer. Summarize the following text, highlighting the key points while retaining important details.\n",
        "Focus on capturing everything needed for a future teaching transcript. Avoid unnecessary fluff, but don't leave out key insights.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_TITLE_INTRO = \"\"\"\n",
        "You are tasked with writing the first part of a structured teaching transcript:\n",
        "1) A compelling Title\n",
        "2) An engaging Introduction\n",
        "\n",
        "Use the text below for context, highlighting relevant details.\n",
        "Provide a strong hook and clarify the purpose of this lecture.\n",
        "Aim for thoroughness (at least a few hundred words).\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_INTRO_OVERVIEW = \"\"\"\n",
        "Now generate the second part of this teaching transcript:\n",
        "Introductory Overview of the main topic.\n",
        "\n",
        "Use the text below for context.\n",
        "Explain background, importance, and scope in a structured, engaging manner.\n",
        "Aim for depth and clarity.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_DETAILED_EXPLANATION = \"\"\"\n",
        "Generate the third part of the teaching transcript:\n",
        "A Detailed Explanation of Key Topics.\n",
        "\n",
        "Use the text below to dive deeper, present subtopics,\n",
        "and cover important points in a systematic way.\n",
        "Include enough detail for a comprehensive understanding.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_PRACTICAL_EXAMPLES = \"\"\"\n",
        "Generate the fourth part of the teaching transcript:\n",
        "Practical Examples and Solutions.\n",
        "\n",
        "Use the text below to illustrate real-world examples,\n",
        "use cases, and best practices.\n",
        "Highlight practical strategies or solutions relevant to the topic.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_SUMMARY_NEXTSTEPS = \"\"\"\n",
        "Finally, generate the fifth part of the transcript:\n",
        "Summary and Next Steps.\n",
        "\n",
        "Use the text below to summarize key takeaways,\n",
        "and provide guidance or next steps for students to continue learning.\n",
        "\"\"\"\n",
        "\n",
        "ENRICH_PROMPT = \"\"\"\n",
        "We have a teaching transcript that isn't quite long enough. Expand it into a more detailed lecture:\n",
        "Aim for 3900+ words. Add further examples, deeper explanations, relevant stories, or advanced topics.\n",
        "\"\"\"\n",
        "\n",
        "#######################################\n",
        "# 4) State Variables for Gradio\n",
        "#######################################\n",
        "parsed_chunks_state = gr.State([])        # Holds chunked documents\n",
        "summary_state = gr.State(\"\")             # Holds combined summary text\n",
        "teaching_transcript_state = gr.State(\"\") # Holds the final teaching transcript\n",
        "\n",
        "#######################################\n",
        "# 5) Helper Functions\n",
        "#######################################\n",
        "def chatgpt_call(system_prompt: str, user_prompt: str) -> str:\n",
        "    messages = [\n",
        "        SystemMessage(content=system_prompt.strip()),\n",
        "        HumanMessage(content=user_prompt.strip())\n",
        "    ]\n",
        "    response = llm(messages)\n",
        "    return response.content.strip()\n",
        "\n",
        "def parse_and_chunk(pdf_file):\n",
        "    \"\"\"Load the PDF and split into chunked documents.\"\"\"\n",
        "    loader = PyPDFLoader(pdf_file.name)\n",
        "    raw_docs = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=3000,\n",
        "        chunk_overlap=500\n",
        "    )\n",
        "    chunked_docs = text_splitter.split_documents(raw_docs)\n",
        "\n",
        "    print(f\"[INFO] Loaded {len(raw_docs)} PDF pages.\")\n",
        "    print(f\"[INFO] Created {len(chunked_docs)} chunks.\")\n",
        "    for i, doc in enumerate(chunked_docs[:3]):\n",
        "        print(f\"[DEBUG] Sample Chunk {i+1} length: {len(doc.page_content.split())} words\")\n",
        "\n",
        "    return chunked_docs\n",
        "\n",
        "def generate_summary(chunks):\n",
        "    \"\"\"\n",
        "    Summarize each chunk individually, then concatenate those partial summaries\n",
        "    into a single \"combined text.\" No extra summarization pass, so we retain detail.\n",
        "    \"\"\"\n",
        "    partial_summaries = []\n",
        "    for doc in chunks:\n",
        "        text = doc.page_content.strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        chunk_summary = chatgpt_call(SUMMARY_PROMPT, text)\n",
        "        partial_summaries.append(chunk_summary)\n",
        "\n",
        "    combined_text = \"\\n\".join(partial_summaries)\n",
        "    print(\"[DEBUG] Combined summary length:\", len(combined_text.split()), \"words\")\n",
        "    return combined_text\n",
        "\n",
        "########################################\n",
        "# Splitting Text (Subchunks) for Map-Reduce\n",
        "########################################\n",
        "def split_text_into_subchunks(text: str, chunk_size=3000, overlap=500) -> list:\n",
        "    subchunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        subchunk = text[start:end]\n",
        "        subchunks.append(subchunk)\n",
        "        start += chunk_size - overlap\n",
        "    return subchunks\n",
        "\n",
        "def generate_section_chunked(full_text: str, system_prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Break the text into sub-chunks, produce partial outputs, then unify them.\n",
        "    \"\"\"\n",
        "    chunk_size = 3000\n",
        "    overlap = 500\n",
        "    subchunks = split_text_into_subchunks(full_text, chunk_size, overlap)\n",
        "\n",
        "    partial_outputs = []\n",
        "    for i, subchunk in enumerate(subchunks):\n",
        "        partial_result = chatgpt_call(system_prompt, subchunk)\n",
        "        partial_outputs.append(partial_result)\n",
        "\n",
        "    joined_text = \"\\n\".join(partial_outputs)\n",
        "    unify_prompt = f\"\"\"\n",
        "We have multiple partial outputs for this section:\n",
        "'{system_prompt.strip()}'\n",
        "Please unify them into one coherent section without omitting important details,make it long, dont summarize but enrichen.\n",
        "\"\"\"\n",
        "    unified_section = chatgpt_call(unify_prompt, joined_text)\n",
        "    return unified_section\n",
        "\n",
        "def generate_structured_transcript(full_combined_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Create a 5-part structured teaching transcript, each part chunked if needed.\n",
        "    Then we simply return the concatenated sections (no final unify pass).\n",
        "    \"\"\"\n",
        "    print(\"[INFO] Generating Title & Intro...\")\n",
        "    section1 = generate_section_chunked(full_combined_text, PROMPT_TITLE_INTRO)\n",
        "\n",
        "    print(\"[INFO] Generating Introductory Overview...\")\n",
        "    section2 = generate_section_chunked(full_combined_text, PROMPT_INTRO_OVERVIEW)\n",
        "\n",
        "    print(\"[INFO] Generating Detailed Explanation...\")\n",
        "    section3 = generate_section_chunked(full_combined_text, PROMPT_DETAILED_EXPLANATION)\n",
        "\n",
        "    print(\"[INFO] Generating Practical Examples & Solutions...\")\n",
        "    section4 = generate_section_chunked(full_combined_text, PROMPT_PRACTICAL_EXAMPLES)\n",
        "\n",
        "    print(\"[INFO] Generating Summary & Next Steps...\")\n",
        "    section5 = generate_section_chunked(full_combined_text, PROMPT_SUMMARY_NEXTSTEPS)\n",
        "\n",
        "    # Combine them directly (NO final unify pass => keep more detail).\n",
        "    all_sections = \"\\n\\n\".join([section1, section2, section3, section4, section5])\n",
        "    print(\"[DEBUG] All sections length:\", len(all_sections.split()), \"words\")\n",
        "\n",
        "    return all_sections\n",
        "\n",
        "########################################\n",
        "# Chunked Enrichment\n",
        "########################################\n",
        "def enrich_transcript_chunked(transcript: str) -> str:\n",
        "    \"\"\"\n",
        "    Break the final transcript into sub-chunks, enrich each chunk, then unify them.\n",
        "    This avoids a single huge prompt that might truncate.\n",
        "    \"\"\"\n",
        "    chunk_size = 3000\n",
        "    overlap = 500\n",
        "    subchunks = split_text_into_subchunks(transcript, chunk_size, overlap)\n",
        "\n",
        "    partial_enrichments = []\n",
        "    for i, sc in enumerate(subchunks):\n",
        "        # \"Map\" step: Enrich each partial chunk\n",
        "        partial = chatgpt_call(ENRICH_PROMPT, sc)\n",
        "        partial_enrichments.append(partial)\n",
        "\n",
        "    # \"Reduce\" step: unify partial enriched chunks\n",
        "    joined_enriched = \"\\n\".join(partial_enrichments)\n",
        "    return joined_enriched\n",
        "\n",
        "#######################################\n",
        "# 6) Gradio Event Handlers\n",
        "#######################################\n",
        "def step1_parse(pdf):\n",
        "    print(\"[INFO] Step 1: Parsing & Chunking started...\")\n",
        "    chunks = parse_and_chunk(pdf)\n",
        "    parsed_chunks_state.value = chunks\n",
        "    print(\"[INFO] Step 1: Parsing & Chunking finished.\")\n",
        "    return (\n",
        "        f\"**Step 1 Results**\\n\\n\"\n",
        "        f\"- Successfully parsed PDF\\n\"\n",
        "        f\"- Found **{len(chunks)}** chunks in total.\"\n",
        "    )\n",
        "\n",
        "def step2_summary():\n",
        "    print(\"[INFO] Step 2: Summarization started...\")\n",
        "    chunks = parsed_chunks_state.value\n",
        "    if not chunks:\n",
        "        print(\"[ERROR] No chunks found.\")\n",
        "        return \"No chunks found. Please parse a PDF first.\"\n",
        "\n",
        "    summary_text = generate_summary(chunks)\n",
        "    summary_state.value = summary_text\n",
        "\n",
        "    word_count = len(summary_text.split())\n",
        "    print(\"[INFO] Step 2: Summarization finished. Word Count:\", word_count)\n",
        "\n",
        "    return (\n",
        "        f\"**Initial Combined Text**\\n\\n{summary_text}\\n\\n\"\n",
        "        f\"**Word Count:** {word_count}\\n\\n\"\n",
        "        \"Use Step 3 to build a multi-step teaching transcript from this text.\"\n",
        "    )\n",
        "\n",
        "def step3_teaching():\n",
        "    # Use the large, combined text\n",
        "    combined_text = summary_state.value\n",
        "    if not combined_text:\n",
        "        print(\"[ERROR] No combined text found.\")\n",
        "        return \"No combined text found. Please run Step 2 first.\"\n",
        "\n",
        "    print(\"[INFO] Generating structured teaching transcript (5 sections, chunked)...\")\n",
        "    transcript = generate_structured_transcript(combined_text)\n",
        "    teaching_transcript_state.value = transcript\n",
        "\n",
        "    word_count = len(transcript.split())\n",
        "    out_text = (\n",
        "        f\"**Teaching Transcript (~30 minutes)**\\n\\n{transcript}\\n\\n\"\n",
        "        f\"**Word Count:** {word_count}\\n\"\n",
        "    )\n",
        "    if word_count < 3900:\n",
        "        out_text += \"\\nIt looks shorter than 3900 words. You can click 'Enrich Transcript' to expand it.\"\n",
        "    else:\n",
        "        out_text += \"\\nGreat! This is around or above 3900 words.\"\n",
        "    return out_text\n",
        "\n",
        "def step4_enrich():\n",
        "    print(\"[INFO] Step 4: Enrichment started...\")\n",
        "    transcript = teaching_transcript_state.value\n",
        "    if not transcript:\n",
        "        print(\"[ERROR] No teaching transcript found.\")\n",
        "        return \"No teaching transcript found. Please run Step 3 first.\"\n",
        "\n",
        "    # Perform chunked enrichment\n",
        "    enriched = enrich_transcript_chunked(transcript)\n",
        "    teaching_transcript_state.value = enriched\n",
        "\n",
        "    word_count = len(enriched.split())\n",
        "    print(\"[INFO] Step 4: Enrichment finished. Word Count:\", word_count)\n",
        "    out_text = (\n",
        "        f\"**Enriched Teaching Transcript**\\n\\n{enriched}\\n\\n\"\n",
        "        f\"**Word Count:** {word_count}\\n\"\n",
        "    )\n",
        "    if word_count < 3900:\n",
        "        out_text += \"\\nStill less than 3900 words. You can enrich again or tweak your prompt.\"\n",
        "    else:\n",
        "        out_text += \"\\nNow it's at or above 3900 words!\"\n",
        "    return out_text\n",
        "\n",
        "#######################################\n",
        "# 7) Gradio Interface\n",
        "#######################################\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Transform Transcript into Teaching Material (Multi-Section, No Final Summarize)\")\n",
        "    gr.Markdown(\n",
        "        \"1) Parse & chunk PDF\\n\"\n",
        "        \"2) Generate an initial combined text (partial summaries joined)\\n\"\n",
        "        \"3) Generate a multi-step teaching transcript (~30 mins) in 5 sections\\n\"\n",
        "        \"4) Enrich the final transcript (chunked) if under 3900 words\"\n",
        "    )\n",
        "\n",
        "    pdf_input = gr.File(label=\"Upload PDF Transcript\")\n",
        "\n",
        "    parse_button = gr.Button(\"Step 1: Parse & Chunk\")\n",
        "    parse_output = gr.Markdown()\n",
        "    parse_button.click(fn=step1_parse, inputs=pdf_input, outputs=parse_output)\n",
        "\n",
        "    summary_button = gr.Button(\"Step 2: Generate Combined Text\")\n",
        "    summary_output = gr.Markdown()\n",
        "    summary_button.click(fn=step2_summary, inputs=None, outputs=summary_output)\n",
        "\n",
        "    teaching_button = gr.Button(\"Step 3: Build Structured Transcript (~30 min)\")\n",
        "    teaching_output = gr.Markdown()\n",
        "    teaching_button.click(fn=step3_teaching, inputs=None, outputs=teaching_output)\n",
        "\n",
        "    enrich_button = gr.Button(\"Step 4: Enrich (Chunked) If Under 3900 Words\")\n",
        "    enrich_output = gr.Markdown()\n",
        "    enrich_button.click(fn=step4_enrich, inputs=None, outputs=enrich_output)\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "872oPNlt-m2m",
        "outputId": "6dbc180f-0ac3-4c75-b1cb-365db4651cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://daf4db8a59c60c40cf.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://daf4db8a59c60c40cf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Step 1: Parsing & Chunking started...\n",
            "[INFO] Loaded 13 PDF pages.\n",
            "[INFO] Created 25 chunks.\n",
            "[DEBUG] Sample Chunk 1 length: 405 words\n",
            "[DEBUG] Sample Chunk 2 length: 113 words\n",
            "[DEBUG] Sample Chunk 3 length: 422 words\n",
            "[INFO] Step 1: Parsing & Chunking finished.\n",
            "[INFO] Step 2: Summarization started...\n",
            "[DEBUG] Combined summary length: 2805 words\n",
            "[INFO] Step 2: Summarization finished. Word Count: 2805\n",
            "[INFO] Generating structured teaching transcript (5 sections, chunked)...\n",
            "[INFO] Generating Title & Intro...\n",
            "[INFO] Generating Introductory Overview...\n",
            "[INFO] Generating Detailed Explanation...\n",
            "[INFO] Generating Practical Examples & Solutions...\n",
            "[INFO] Generating Summary & Next Steps...\n",
            "[DEBUG] All sections length: 2114 words\n",
            "[INFO] Step 4: Enrichment started...\n",
            "[INFO] Step 4: Enrichment finished. Word Count: 8882\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://daf4db8a59c60c40cf.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}